{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97ed5bbc-c2db-43dd-8768-852df790c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa94b4-ef22-4a2b-8c7f-fe0c3ce41af7",
   "metadata": {},
   "source": [
    "# Aim: Perform Batch Gradient Descent with Early stopping for Softmax Regression without scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382d4a1-793b-4c6b-9c98-175ca22c58c3",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e9ec27-45e3-44bc-82b2-df885e51fd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename',\n",
       " 'data_module']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris(as_frame=True) # load data as a dataframe\n",
    "\n",
    "list(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0e0203f-bc2a-432c-93e8-507d19341262",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"].values   # get data as numpy array\n",
    "\n",
    "y = iris[\"target\"].values # get target label as numpy array \n",
    "\n",
    "X_with_bias = np.c_[np.ones(len(X)), X] # (or) easier method - add_dummy_feature(X)\n",
    "\n",
    "n_inp_features = X_train.shape[1] # 3 featues and 1 bias term - input\n",
    "n_out_classes  = len(np.unique(y)) # 3 output classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a66cd-29c1-4dc6-8e60-73be865a4791",
   "metadata": {},
   "source": [
    "### 2. Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94d25663-b7a2-4c6b-a9f7-ad01caadc3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2                           # test ratio to total size\n",
    "total_size = len(X_with_bias)\n",
    "\n",
    "test_size = int(test_ratio * total_size)  # number of training data\n",
    "train_size = total_size - test_size       # number of testing data \n",
    "\n",
    "np.random.seed(42)\n",
    "rdn_idx = np.random.permutation(total_size) # shuffle from 0 to 149\n",
    "\n",
    "X_train = X_with_bias[rdn_idx[:train_size]] # training data\n",
    "X_test = X_with_bias[rdn_idx[train_size:]] # test data \n",
    "y_train = y[rdn_idx[:train_size]]          # training data label\n",
    "y_test = y[rdn_idx[train_size:]]           # testing data label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8887fdf-aab5-4797-926f-6906354ffeff",
   "metadata": {},
   "source": [
    "### 3. Encoding\n",
    "Since softmax regreesion is to be done on target class probabilities that is the values need to be 0 or 1, the target indices needs to be encoded. Therefore, one hot encoding needs to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e05fea29-dadf-49f9-8352-a4ef61078bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    n_cats = np.ones(y.max()+1) # number of categories    \n",
    "    cat_diag = np.diag(n_cats) # diagonalise the categories\n",
    "    \n",
    "    return cat_diag[y]        # encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "462874e2-7a5c-4ee5-b7ab-c1e743c777a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = one_hot(y_train)\n",
    "y_test_enc = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a320f9e-13b5-403a-a23c-7695648361c3",
   "metadata": {},
   "source": [
    "### 4. Scaling\n",
    "Scaling neeeds to be done for the GD to work rpoperly and give a values without any bias. Perform `Z-transform`, that is\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c08a95b8-93bf-4e75-99a0-9e358ab7c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean    = X_train[:, 1:].mean(axis=0) # scale all features except bias term\n",
    "std_dev = X_train[:, 1:].std(axis=0)\n",
    "\n",
    "X_train[:, 1:] = (X_train[:,1:] - mean)/std_dev # z formula\n",
    "\n",
    "mean    = X_test[:, 1:].mean(axis=0) # scale all features except bias term\n",
    "std_dev = X_test[:, 1:].std(axis=0)\n",
    "\n",
    "X_test[:, 1:] = (X_test[:,1:] - mean)/std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17143f-0700-46b1-ab32-509a843e52d3",
   "metadata": {},
   "source": [
    "### 5. Training\n",
    "Softmax function is of the form,\n",
    "$$p_i = S(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_i}}$$, p - vector of probability of each class\n",
    "\n",
    "The loss function for softmax is of the form,\n",
    "$$J(\\mathbf{\\Theta}) = -\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$$\n",
    "\n",
    "The gradient of the loss function is of the form,\n",
    "$$\\nabla_\\theta J(\\mathbf{\\Theta}) = \\frac{1}{m}\\sum\\limits_{i=1}^{m}(\\hat{p}_k^{(i)} - y_k^{(i)})x^{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5e2e8ff2-8d5f-4159-9322-391708637d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    numer = np.exp(logits) # exponential of each term\n",
    "\n",
    "    denom = numer.sum(axis=1, keepdims=True) # keepdims - required for broadcasting\n",
    "\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2a99b118-b08d-4c38-8b4f-3b1d4ebb000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch : 0 | loss : 1.2910169335356907\n",
      "At epoch : 100 | loss : 0.14209276578243565\n",
      "At epoch : 200 | loss : 0.11259462605070966\n",
      "At epoch : 300 | loss : 0.0968287305479978\n",
      "At epoch : 400 | loss : 0.08696449280252114\n",
      "At epoch : 500 | loss : 0.08032410664891683\n",
      "At epoch : 600 | loss : 0.07559923941956628\n",
      "At epoch : 700 | loss : 0.07208461235987478\n",
      "At epoch : 800 | loss : 0.06937447461566741\n",
      "At epoch : 900 | loss : 0.0672223505399955\n",
      "At epoch : 1000 | loss : 0.0654713951182708\n",
      "At epoch : 1100 | loss : 0.06401775070213839\n",
      "At epoch : 1200 | loss : 0.06279025581422021\n",
      "At epoch : 1300 | loss : 0.06173868967763608\n",
      "At epoch : 1400 | loss : 0.06082668579165255\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1                    # learning rate\n",
    "epochs = 1500               # number of epochs\n",
    "m = train_size              # to compute the average\n",
    "epsilon = 1e-5\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(n_inp_features, n_out_classes) # 5X3 random initial parameter value\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logits = X_train @ theta # logits - output of normal linear regression\n",
    "\n",
    "    y_prob = softmax(logits) # softmax of y\n",
    "\n",
    "    if epoch % 100 == 0 or epochs <= 10:\n",
    "        y_prob_test = softmax(X_test @ theta)\n",
    "\n",
    "        test_cross_entropy_loss = -(y_test_enc * np.log(y_prob_test + epsilon)).mean() # epsilon - to prevent from getting NaN if pprobability is 0\n",
    "\n",
    "        print(f\"At epoch : {epoch} | loss : {test_cross_entropy_loss}\")\n",
    "\n",
    "    err = y_prob - y_train_enc    # error - offset away from actual\n",
    "\n",
    "    grads = 1/m * X_train.T @ err # gradient\n",
    "\n",
    "    theta -= lr * grads           # step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "31f3d819-2d7a-4962-8418-b1168d7f642e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10123407,  2.75412715, -1.84922283],\n",
       "       [-1.11124607,  1.03434243,  1.13164316],\n",
       "       [ 2.43081281,  0.02427266, -0.57791232],\n",
       "       [-2.62725075, -0.0571928 ,  2.29785615],\n",
       "       [-2.92640378, -2.36350271,  1.89367068]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta # fitted model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d4d590-cb21-460e-bf32-f0470bb65a1a",
   "metadata": {},
   "source": [
    "### 6. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9e98030b-97cd-4b48-bdfd-f71c5bd7bc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(1), np.int64(1))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred = X_test[5].reshape(-1,1)\n",
    "y_hat = y_test[5]\n",
    "\n",
    "y_prob = X_pred.T @ theta # prediccted probability of each class\n",
    "\n",
    "y_pred = np.argmax(y_prob)\n",
    "y_pred, y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6534e-2eb3-4698-b0cd-6dd228e1927d",
   "metadata": {},
   "source": [
    "### 7. Accuracy Score\n",
    "Lets test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3458bad8-4922-44a4-a1d2-b3131900b198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 90.0%\n"
     ]
    }
   ],
   "source": [
    "y_prob = X_test @ theta \n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "accuracy = (y_pred == y_test).sum()/len(y_test) * 100 # accuracy formula - or (y_pred == y_test).mean()\n",
    "print(f\"Accuracy is {accuracy.item()}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca710b-40a1-4cf1-95c8-d8adcedd90c3",
   "metadata": {},
   "source": [
    "# Additional\n",
    "\n",
    "### a.if regularisation is done (L2 is perfomred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5a976479-f923-4a62-8bf7-5fdef6c0bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch : 0 | loss : 3.9388163174414963\n",
      "At epoch : 100 | loss : 0.48123501892987247\n",
      "At epoch : 200 | loss : 0.4062102320253216\n",
      "At epoch : 300 | loss : 0.3712593362241967\n",
      "At epoch : 400 | loss : 0.35361467575107003\n",
      "At epoch : 500 | loss : 0.34524670346484576\n",
      "At epoch : 600 | loss : 0.3421836211532245\n",
      "At epoch : 700 | loss : 0.34229269753743674\n",
      "At epoch : 800 | loss : 0.3443648743170089\n",
      "At epoch : 900 | loss : 0.34768247627464144\n",
      "At epoch : 1000 | loss : 0.35180249649327805\n",
      "At epoch : 1100 | loss : 0.3564423385762572\n",
      "At epoch : 1200 | loss : 0.3614167437752964\n",
      "At epoch : 1300 | loss : 0.36660150829610083\n",
      "At epoch : 1400 | loss : 0.37191183641452463\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1                    # learning rate\n",
    "epochs = 1500               # number of epochs\n",
    "m = train_size              # to compute the average\n",
    "epsilon = 1e-5\n",
    "alpha = 0.01                # regularisation hyperparameter\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(n_inp_features, n_out_classes) # 5X3 random initial parameter value\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logits = X_train @ theta # logits - output of normal linear regression\n",
    "\n",
    "    y_prob = softmax(logits) # softmax of y\n",
    "\n",
    "    if epoch % 100 == 0 or epochs <= 10:\n",
    "        y_prob_test = softmax(X_test @ theta)\n",
    "\n",
    "        test_cross_entropy_loss = -(y_test_enc * np.log(y_prob_test + epsilon)) # epsilon - to prevent from getting NaN if pprobability is 0\n",
    "\n",
    "        l2_loss = 1/2 * (theta[1:]**2).sum() # to be done on all featues except bias\n",
    "\n",
    "        total_loss = test_cross_entropy_loss.sum(axis=1).mean() + alpha * l2_loss\n",
    "\n",
    "        print(f\"At epoch : {epoch} | loss : {total_loss}\")\n",
    "\n",
    "    err = y_prob - y_train_enc    # error - offset away from actual\n",
    "\n",
    "    grads = 1/m * X_train.T @ err # gradient\n",
    "\n",
    "    grads += np.r_[np.zeros([1,n_out_classes]), alpha/m * theta[1:]] # concatenate along axis0 to add reg value to all params\n",
    "\n",
    "    theta -= lr * grads           # step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a00947a4-7e34-4d27-ba93-bd9f81bfc42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 90.0%\n"
     ]
    }
   ],
   "source": [
    "y_prob = X_test @ theta \n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "accuracy = (y_pred == y_test).sum()/len(y_test) * 100 # accuracy formula - or (y_pred == y_test).mean()\n",
    "print(f\"Accuracy is {accuracy.item()}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21944869-463d-45e4-962a-6def83faa69b",
   "metadata": {},
   "source": [
    "### b. Early stopping\n",
    "Tuning the hyperparameter alpha doesn;t seem to vary the accuracy, therefore lets try tuning the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d8a17e8d-a7cf-4273-b1ae-702418b183bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch : 0 | loss : 3.9388163174414963\n",
      "At epoch : 100 | loss : 0.48123501892987247\n",
      "At epoch : 200 | loss : 0.4062102320253216\n",
      "At epoch : 300 | loss : 0.3712593362241967\n",
      "At epoch : 400 | loss : 0.35361467575107003\n",
      "At epoch : 500 | loss : 0.34524670346484576\n",
      "At epoch : 600 | loss : 0.3421836211532245\n",
      "644 0.3419 early stopping!\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1                    # learning rate\n",
    "epochs = 1500               # number of epochs\n",
    "m = train_size              # to compute the average\n",
    "epsilon = 1e-5\n",
    "alpha = 0.01                # regularisation hyperparameter\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(n_inp_features, n_out_classes) # 5X3 random initial parameter value\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logits = X_train @ theta # logits - output of normal linear regression\n",
    "\n",
    "    y_prob = softmax(logits) # softmax of y\n",
    "\n",
    "    y_prob_test = softmax(X_test @ theta)\n",
    "\n",
    "    test_cross_entropy_loss = -(y_test_enc * np.log(y_prob_test + epsilon)) # epsilon - to prevent from getting NaN if pprobability is 0\n",
    "\n",
    "    l2_loss = 1/2 * (theta[1:]**2).sum() # to be done on all featues except bias\n",
    "\n",
    "    total_loss = test_cross_entropy_loss.sum(axis=1).mean() + alpha * l2_loss\n",
    "\n",
    "    if epoch % 100 == 0 or epochs <= 10:\n",
    "        print(f\"At epoch : {epoch} | loss : {total_loss}\")\n",
    "    \n",
    "    if total_loss < best_loss: # condition for early stopping\n",
    "        best_loss = total_loss\n",
    "\n",
    "    else:\n",
    "        print(epoch-1, np.round(best_loss,4), \"early stopping!\")\n",
    "        break\n",
    "\n",
    "    err = y_prob - y_train_enc    # error - offset away from actual\n",
    "\n",
    "    grads = 1/m * X_train.T @ err # gradient\n",
    "\n",
    "    grads += np.r_[np.zeros([1,n_out_classes]), alpha/m * theta[1:]] # concatenate along axis0 to add reg value to all params\n",
    "\n",
    "    theta -= lr * grads           # step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3c85f970-54e8-43ba-b780-77f7f36fb636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 93.33333333333333%\n"
     ]
    }
   ],
   "source": [
    "y_prob = X_test @ theta \n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "accuracy = (y_pred == y_test).sum()/len(y_test) * 100 # accuracy formula - or (y_pred == y_test).mean()\n",
    "print(f\"Accuracy is {accuracy.item()}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95854f-d8ea-4f3b-86ac-89709ec0f017",
   "metadata": {},
   "source": [
    "We can see that there is an increase in performance by doing early stopping. Therefore by early stopping, overfitting of the model is prevented"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
